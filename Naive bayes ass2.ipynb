{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf3f65a-a79c-48ad-bcc2-a34e1a9d59ac",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc6ab3-0689-4e56-9a19-6a34d1535e2e",
   "metadata": {},
   "source": [
    "We are asked to find the probability that an employee is a smoker given that they use the company's health insurance plan. This is a conditional probability problem that can be solved using Bayes' theorem.\n",
    "\n",
    "Given:\n",
    "\n",
    "P(H)=0.70: The probability that an employee uses the health insurance plan.\n",
    "\n",
    "P(S∣H)=0.40: The probability that an employee is a smoker given they use the health insurance plan.\n",
    "\n",
    "We need to find \n",
    "\n",
    "P(S∣H), which is directly given as 0.40.\n",
    "\n",
    "Solution:\n",
    "\n",
    "The probability that an employee is a smoker given that they use the health insurance plan is 0.40 or 40%.\n",
    "\n",
    "Thus, the probability is 0.40 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d0c98-3101-4b31-8e2a-7787a0bf2ba2",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f746d63-b0fb-4a89-8546-1586ec1692b9",
   "metadata": {},
   "source": [
    "The Bernoulli Naive Bayes and Multinomial Naive Bayes classifiers are both types of Naive Bayes models, but they differ in how they handle data and their underlying assumptions. Here’s a comparison of the two:\n",
    "\n",
    "1. Type of Input Data:\n",
    "\n",
    "### Bernoulli Naive Bayes:\n",
    "\n",
    "Deals with binary or Boolean data.\n",
    "\n",
    "Each feature represents whether a particular attribute is present or absent (e.g., whether a word appears in a document or not).\n",
    "\n",
    "Example: In text classification, the features are 0 (word absent) or 1 (word present).\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "\n",
    "Designed for discrete or count-based data.\n",
    "\n",
    "Each feature represents the frequency or count of an attribute (e.g., the number of times a word appears in a document).\n",
    "\n",
    "Example: In text classification, the features represent word counts or frequencies in documents.\n",
    "\n",
    "2. Likelihood Function:\n",
    "\n",
    "### Bernoulli Naive Bayes:\n",
    "\n",
    "Assumes that features are binary (either 0 or 1), and each feature follows a Bernoulli distribution.\n",
    "\n",
    "Suitable for tasks where you only care about the presence or absence of a feature.\n",
    "\n",
    "Penalizes the model for a feature that is absent in the training set but present in the test set.\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "\n",
    "Assumes that features are discrete counts, and the features follow a multinomial distribution.\n",
    "\n",
    "Focuses on the number of occurrences of each feature (e.g., word frequency).\n",
    "\n",
    "Does not penalize the model for unseen features, making it more flexible in handling sparse data like text documents.\n",
    "\n",
    "3. Typical Use Cases:\n",
    "\n",
    "### Bernoulli Naive Bayes:\n",
    "\n",
    "Used when features are binary or represent the presence/absence of something.\n",
    "\n",
    "Best suited for text classification tasks where you're only interested in whether words occur or not, such as:\n",
    "Spam filtering (whether specific words are present in an email).\n",
    "\n",
    "Document classification with binary word presence features.\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "\n",
    "Used when features are counts or frequencies.\n",
    "\n",
    "Best suited for text classification tasks that involve counting how often words appear in a document, such as:\n",
    "Sentiment analysis (analyzing how many times positive or negative words appear in a review).\n",
    "\n",
    "Document categorization where word frequencies are key.\n",
    "\n",
    "4. Handling Absence of Features:\n",
    "\n",
    "### Bernoulli Naive Bayes:\n",
    "\n",
    "Explicitly penalizes for the absence of a feature when it was expected to be present, as it models both presence (1) and absence (0) explicitly.\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "\n",
    "Does not penalize for the absence of features in the same way; it focuses on how many times features (e.g., words) appear.\n",
    "\n",
    "Summary:\n",
    "\n",
    "Bernoulli Naive Bayes: Suitable for binary data (e.g., presence/absence of words in text classification).\n",
    "\n",
    "Multinomial Naive Bayes: Suitable for count data (e.g., word frequency in text classification).\n",
    "\n",
    "Both classifiers are widely used in text classification but are suited to different types of input data and specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7c78e-00e4-4e1c-b349-409da9471241",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772853d5-6234-4f40-89a1-6ba966ba26fd",
   "metadata": {},
   "source": [
    "In Bernoulli Naive Bayes, missing values can be thought of as features that are neither explicitly 0 (absent) nor 1 (present). However, the model inherently expects binary data for each feature. Here's how it handles missing values:\n",
    "\n",
    "1. Assumption of Binary Features:\n",
    "\n",
    "Bernoulli Naive Bayes assumes that every feature takes a binary value (either 0 or 1), indicating the absence or presence of a particular attribute. Missing values are not naturally handled in the model’s design because it expects a complete set of binary values for each feature.\n",
    "\n",
    "2. Strategies to Handle Missing Values:\n",
    "\n",
    "To deal with missing values in a Bernoulli Naive Bayes model, the missing data must be preprocessed. Common strategies include:\n",
    "\n",
    "Imputation: Replace missing values with either 0 or 1 based on prior knowledge or statistical methods.\n",
    "\n",
    "Mean/Mode Imputation: If most instances of a feature are 1 (present), missing values might be imputed with 1. Similarly, for features mostly absent (0), impute missing values with 0.\n",
    "\n",
    "Custom Imputation: Use domain-specific knowledge to infer whether the feature is likely present or absent when missing.\n",
    "\n",
    "Ignore Missing Features: In some cases, missing values can be ignored during training and testing by simply treating them as if they were absent (0). This is particularly common if the missing value can reasonably be interpreted as \"absence of data.\"\n",
    "\n",
    "Use Probabilities: Assign a probability for the presence or absence of the feature when it's missing, based on the distribution of that feature in the training data.\n",
    "\n",
    "3. Impact of Missing Values:\n",
    "\n",
    "Missing as 0: In many text classification tasks, a missing value can often be interpreted as \"absence\" (0), which fits naturally into the Bernoulli Naive Bayes model (e.g., if a word doesn't appear in a document, it’s treated as 0).\n",
    "\n",
    "Over-penalization: If missing values are incorrectly handled, the model might penalize instances too much for features that are absent in the training data but present in test data, or vice versa.\n",
    "\n",
    "4. Alternative Handling in Data Preparation:\n",
    "\n",
    "If missing values are frequent and cannot easily be interpreted as absent, you may need to preprocess the data using imputation techniques or consider using a model better suited to handle missing data, such as decision trees or ensemble methods like random forests.\n",
    "\n",
    "Summary:\n",
    "\n",
    "Bernoulli Naive Bayes does not handle missing values natively.\n",
    "\n",
    "You need to preprocess the data, either by imputing missing values (with 0 or 1) or ignoring them in the context of training/testing.\n",
    "\n",
    "The interpretation of missing values as \"absence\" (0) is common, especially in text-based tasks, but this approach should be chosen based on the specific context of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a280a8-4d89-4f01-bad6-386bd319e801",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a988b0e3-d28d-49fd-80e1-ce46ef78db82",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. It is not limited to binary classification and works effectively with problems that have more than two classes.\n",
    "\n",
    "### How Gaussian Naive Bayes Handles Multi-Class Classification:\n",
    "\n",
    "1. Class-Conditional Distributions:\n",
    "Gaussian Naive Bayes assumes that the continuous features are normally distributed for each class. For multi-class problems, it estimates a Gaussian distribution (mean and variance) for each feature within each class.\n",
    "\n",
    "2. Classification: \n",
    "During classification, the model computes the probability of the input belonging to each class using Bayes’ theorem. The class with the highest posterior probability is chosen as the predicted class.\n",
    "\n",
    "Steps for Multi-Class Classification:\n",
    "\n",
    "Training: The model calculates the mean and variance of each feature for every class.\n",
    "\n",
    "Prediction: For a new instance, the probability of that instance belonging to each class is computed using the Gaussian probability density function for each feature. The prior probability of each class is also considered.\n",
    "\n",
    "Class Selection: The class with the highest posterior probability is selected as the predicted class.\n",
    "\n",
    "### Example:\n",
    "Suppose we have a dataset with three classes: Class A, Class B, and Class C. For each class, Gaussian Naive Bayes will:\n",
    "\n",
    "Estimate the mean and variance for each feature conditioned on each class.\n",
    "\n",
    "For a new input, it will calculate the likelihood of the input belonging to each of the three classes based on these Gaussian distributions.\n",
    "\n",
    "The input is then assigned to the class with the highest posterior probability.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Gaussian Naive Bayes is well-suited for multi-class classification tasks where the features are continuous and can be assumed to follow a normal distribution.\n",
    "\n",
    "It applies the same principles as binary classification, but extends to more than two classes by computing probabilities for each class and selecting the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87ef44-f982-426f-8428-dadfa551f8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d79bdb-3e0e-4643-872d-4fd5c541583d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238bdb42-f888-4cd8-a84e-c157847338eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
